{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies\n",
    "\n",
    "Import necessary libraries and frameworks:\n",
    "- matplotlib: For data visualization\n",
    "- tensorflow: For building and training neural networks\n",
    "- pandas: For data manipulation\n",
    "- numpy: For numerical operations\n",
    "- keras: High-level neural network API\n",
    "Additional imports for image processing and model building components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "import os\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path to your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'PetImages'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Processing Function\n",
    "\n",
    "Define a function to process and clean the image dataset:\n",
    "- Converts all images to RGB format\n",
    "- Removes corrupted images\n",
    "- Handles exceptions during processing\n",
    "Parameters:\n",
    "    folder_path (str): Path to the main dataset directory containing Cat and Dog subdirectories\n",
    "Returns:\n",
    "    None, but prints processing statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images_to_rgb(folder_path):\n",
    "    processed_count = 0\n",
    "    removed_count = 0\n",
    "\n",
    "    for i in [\"Cat\", \"Dog\"]:\n",
    "        path = os.path.join(folder_path, i)\n",
    "        for filename in os.listdir(path):\n",
    "            filepath = os.path.join(path, filename)\n",
    "            try:\n",
    "                # Open the image\n",
    "                with Image.open(filepath) as img:\n",
    "                    # Convert image to RGB if not already in RGB\n",
    "                    if img.mode != \"RGB\":\n",
    "                        print(f\"Converting {filename} to RGB\")\n",
    "                        img = img.convert(\"RGB\")\n",
    "                    img.save(filepath)  # Save the image back in RGB format\n",
    "                processed_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Removing corrupted file: {filename}, Error: {e}\")\n",
    "                os.remove(filepath)\n",
    "                removed_count += 1\n",
    "\n",
    "    print(f\"Processing complete. Processed: {processed_count}, Removed: {removed_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runs the image processing function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_images_to_rgb(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify available classes (Cat and Dog)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = os.listdir(path)\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization\n",
    "\n",
    "Create a visualization grid of sample images:\n",
    "- Displays 8 cat images and 8 dog images\n",
    "- Creates a 4x4 grid of images\n",
    "- Removes axes for better visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16, 16)\n",
    "\n",
    "cat_dir = os.path.join(path, 'cat')\n",
    "dog_dir = os.path.join(path, 'dog')\n",
    "cat_names = os.listdir(cat_dir)\n",
    "dog_names = os.listdir(dog_dir)\n",
    "\n",
    "pic_index = 210\n",
    "\n",
    "cat_images = [os.path.join(cat_dir, fname)\n",
    "              for fname in cat_names[pic_index-8:pic_index]]\n",
    "dog_images = [os.path.join(dog_dir, fname)\n",
    "              for fname in dog_names[pic_index-8:pic_index]]\n",
    "\n",
    "for i, img_path in enumerate(cat_images + dog_images):\n",
    "    sp = plt.subplot(4, 4, i+1)\n",
    "    sp.axis('Off')\n",
    "\n",
    "    img = mpimg.imread(img_path)\n",
    "    plt.imshow(img)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation\n",
    "\n",
    "Create training and testing datasets using TensorFlow's image_dataset_from_directory:\n",
    "Parameters:\n",
    "- image_size: (200,200) for consistent input size\n",
    "- validation_split: 0.1 (10% for validation)\n",
    "- batch_size: 32 images per batch\n",
    "- seed: 1 for reproducibility\n",
    "- subset: 'training' for training data, 'validation' for validation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = image_dataset_from_directory(path,\n",
    "                                                  image_size=(200,200),\n",
    "                                                  subset='training',\n",
    "                                                  seed = 1,\n",
    "                                                  validation_split=0.1,\n",
    "                                                  batch_size= 32)\n",
    "test_datagen = image_dataset_from_directory(path,\n",
    "                                                  image_size=(200,200),\n",
    "                                                  subset='validation',\n",
    "                                                  seed = 1,\n",
    "                                                  validation_split=0.1,\n",
    "                                                  batch_size= 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "\n",
    "Define the CNN model architecture:\n",
    "- 4 Convolutional layers with MaxPooling\n",
    "- Flatten layer to convert 2D features to 1D\n",
    "- 3 Dense layers with BatchNormalization and Dropout\n",
    "- Final sigmoid layer for binary classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(200, 200, 3)),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Compilation\n",
    "\n",
    "Compile the model with:\n",
    "- Binary cross-entropy loss (suitable for binary classification)\n",
    "- Adam optimizer\n",
    "- Accuracy metric for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "Train the model:\n",
    "- Using the training dataset\n",
    "- For 10 epochs\n",
    "- With validation data for performance monitoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_datagen,\n",
    "          epochs=10,\n",
    "          validation_data=test_datagen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Saving\n",
    "\n",
    "Save the trained model to disk in H5 format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"catvsdog.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
